# main.py or data_models.py

from typing import List, Dict, Optional, Any, TypedDict
from pydantic import BaseModel, Field
import json
import re

# --- Configuration (placeholders, to be filled) ---
# We'll need to decide on actual models later. For now, these are conceptual.
# For local development, consider Ollama with models like Llama3 or Phi3 for LLM
# and something like 'all-MiniLM-L6-v2' from sentence-transformers for embeddings.
# For more power, OpenAI API, Anthropic API, etc.

LLM_PROVIDER = "placeholder_llm_provider" # e.g., "openai", "anthropic", "ollama"
EMBEDDING_MODEL_NAME = "placeholder_embedding_model" # e.g., "text-embedding-ada-002", "all-MiniLM-L6-v2"
ASSESSOR_LLM_MODEL = "placeholder_assessor_llm" # e.g., "gpt-4-turbo", "claude-3-opus"
TOPIC_SEGMENTATION_LLM_MODEL = "placeholder_topic_llm" # e.g., "gpt-3.5-turbo", "claude-3-sonnet"

# --- Data Models ---

class DialogueTurn(BaseModel):
    turn_id: int
    speaker: str
    start_timestamp: str = Field(alias="start") # Handles the alias from input JSON
    end_timestamp: str = Field(alias="end")
    raw_text: str = Field(alias="text")
    clean_text: Optional[str] = None
    emotion: Optional[str] = None
    # We might add embeddings later directly here or store them separately mapped by turn_id
    # embedding: Optional[List[float]] = None

class TopicSegment(BaseModel):
    '''segment a series of turns to a certain topic'''
    topic_id: str
    topic_label: str
    start_turn_id: int
    end_turn_id: int
    # We could add a summary of the topic here later, generated by an LLM
    # topic_summary: Optional[str] = None

class RetrievedContext(BaseModel):
    criterion_assessed: str
    topic_label_context: Optional[str] = None # The topic this context primarily belongs to
    dialogue_turns: List[DialogueTurn] # The actual snippet (e.g., Q+A)
    # source_turn_ids: List[int] # IDs of the original turns that form this context

class IndividualAssessment(BaseModel):
    criterion: str
    topic_assessed: Optional[str] = None # Which topic was primarily used for this assessment
    score: Optional[float] = None # e.g., 1-5
    reasoning: str
    evidence: List[str] # Direct quotes from the transcript
    # raw_llm_response: Optional[str] = None # For debugging

class InterviewAssessmentReport(BaseModel):
    overall_summary: Optional[str] = None
    strengths: Optional[List[str]] = None
    areas_for_improvement: Optional[List[str]] = None
    detailed_assessments: List[IndividualAssessment]
    # We could add metadata like interview_id, date_processed, etc.

# Initial Assessment Criteria (can be expanded)
ASSESSMENT_CRITERIA = [
    {
        "id": "clarity",
        "criterion": "Clarity of Communication",
        "description": "Assesses how clearly and concisely the candidate expresses their thoughts and answers questions.",
        "scoring_guide": "1 (Very Unclear) to 5 (Exceptionally Clear)"
    },
    {
        "id": "engagement",
        "criterion": "Engagement and Enthusiasm",
        "description": "Assesses the candidate's level of interest, energy, and enthusiasm displayed during the interview, considering verbal cues and detected emotions.",
        "scoring_guide": "1 (Disengaged) to 5 (Highly Engaged and Enthusiastic)"
    },
    {
        "id": "professionalism",
        "criterion": "Professionalism and Tone",
        "description": "Assesses the candidate's demeanor, use of professional language, and appropriateness of tone and emotion.",
        "scoring_guide": "1 (Unprofessional) to 5 (Highly Professional)"
    },
    # Example of a topic-dependent criterion (will need dynamic handling or specific topic targeting)
    # For V1, we might make problem-solving more general, or assume a primary "problem-solving" topic.
    {
        "id": "problem_solving_approach",
        "criterion": "Problem-Solving Approach Explanation",
        "description": "Assesses how well the candidate explains their approach to solving a relevant problem discussed, focusing on logic and structure of their explanation.",
        "scoring_guide": "1 (Poor Explanation) to 5 (Excellent Explanation)"
    },
    {
        "id": "active_listening",
        "criterion": "Active Listening",
        "description": "Assesses if the candidate listens attentively to the interviewer, addresses questions directly, and references previous points where appropriate.",
        "scoring_guide": "1 (Poor Listener) to 5 (Excellent Listener)"
    }
]


# --- Preprocessing Functions ---

def parse_emotion_from_text(text: str) -> tuple[Optional[str], str]:
    """Extracts emotion tag and returns clean text."""
    match = re.match(r"\[Emotion: ([\w\s]+)\]\s*(.*)", text)
    if match:
        emotion = match.group(1).strip().upper()
        clean_text = match.group(2).strip()
        return emotion, clean_text
    return None, text.strip()

def preprocess_transcript(raw_transcript_data: List[Dict]) -> List[DialogueTurn]:
    """Loads raw transcript, parses emotion, and assigns turn IDs."""
    processed_turns = []
    for i, raw_turn in enumerate(raw_transcript_data):
        emotion, clean_text = parse_emotion_from_text(raw_turn["text"])
        turn = DialogueTurn(
            turn_id=i,
            speaker=raw_turn["speaker"],
            start=raw_turn["start"], # Pydantic handles alias
            end=raw_turn["end"],     # Pydantic handles alias
            text=raw_turn["text"],   # Pydantic handles alias (raw_text field)
            clean_text=clean_text,
            emotion=emotion
        )
        processed_turns.append(turn)
    return processed_turns

# --- Placeholder for LLM and Embedding Clients ---
# We will define these more concretely as we select tools.
# For now, they'll just be conceptual.

'''
class LLMClient:
    def __init__(self, provider: str, model_name: str, api_key: Optional[str] = None):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key
        print(f"Initializing LLMClient: Provider={provider}, Model={model_name}")

    async def generate(self, prompt: str, system_prompt: Optional[str] = None, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        # This will be replaced with actual API calls
        print(f"\n--- LLM Call ({self.model_name}) ---")
        if system_prompt:
            print(f"SYSTEM: {system_prompt}")
        print(f"PROMPT: {prompt[:200]}...") # Print a snippet
        # Mock response for now
        if "segment the following interview dialogue" in prompt.lower():
            # Mocking topic segmentation
            mock_topics = [
                {"topic_id": "T1", "topic_label": "Introductions and Rapport Building", "start_turn_id": 0, "end_turn_id": 2},
                {"topic_id": "T2", "topic_label": "Candidate's Project Experience Discussion", "start_turn_id": 3, "end_turn_id": 7},
                {"topic_id": "T3", "topic_label": "Candidate Questions & Wrap-up", "start_turn_id": 8, "end_turn_id": 9} # Assuming 10 turns total in mock
            ]
            return json.dumps(mock_topics)
        elif "assess the candidate's" in prompt.lower():
            # Mocking individual assessment
            mock_assessment = {
                "score": 4.0,
                "reasoning": "Candidate provided clear examples and demonstrated good understanding.",
                "evidence": ["Quote 1 from dialogue...", "Quote 2 from dialogue..."]
            }
            # The actual prompt will specify the criterion. We need to make sure the mock reflects that for testing.
            # For now, generic.
            return json.dumps(mock_assessment)
        elif "overall interview assessment summary" in prompt.lower():
            # Mocking final summary
            mock_summary = {
                "overall_summary": "The candidate performed well, showing strong communication skills and relevant experience.",
                "strengths": ["Clear communication", "Good project examples"],
                "areas_for_improvement": ["Could provide more detail on future goals"]
            }
            return json.dumps(mock_summary)
        return "This is a mock LLM response."

    async def generate_structured(self, prompt: str, output_model: BaseModel, system_prompt: Optional[str] = None, temperature: float = 0.7, max_tokens: int = 1000) -> BaseModel:
        # This would use something like OpenAI's functions/tools or Instructor library for structured output
        print(f"\n--- Structured LLM Call ({self.model_name}) for {output_model.__name__} ---")
        if system_prompt:
            print(f"SYSTEM: {system_prompt}")
        print(f"PROMPT: {prompt[:200]}...")
        
        # Simplified mock logic for structured output
        if output_model == TopicSegment: # This won't work directly like this for a list
             # This mock is more for conceptual flow; actual implementation would generate a list of TopicSegments
            mock_topics_list_str = await self.generate(prompt, system_prompt, temperature, max_tokens)
            # In a real scenario, you'd parse mock_topics_list_str into List[TopicSegment]
            # For simplicity, we'll assume the generate method already handles JSON list for topics
            # and the caller of generate_structured for topics expects a List[TopicSegment]
            # and handles the parsing of the string response. This is a bit of a kludge for mock.
            # Let's refine the mock for generate() for topics to return a string that can be parsed into List[TopicSegment]
            # For other BaseModels like IndividualAssessment, we can mock a single instance
            try:
                raw_response = await self.generate(prompt, system_prompt, temperature, max_tokens)
                # For TopicSegment, we actually expect a list. This is a simplification.
                # The calling node should handle parsing a list of topics.
                # For a single IndividualAssessment:
                data = json.loads(raw_response)
                return output_model(**data)
            except Exception as e:
                print(f"Mocking Error for structured output: {e}")
                # Fallback to a default instance or raise error
                if output_model == IndividualAssessment:
                     return IndividualAssessment(criterion="Mocked Criterion", reasoning="Mocked reasoning due to error", evidence=[])
                raise
'''
from dotenv import load_dotenv  
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
load_dotenv()
LLMClient = init_chat_model("llama3-8b-8192", model_provider="groq")

'''
class EmbeddingClient:
    def __init__(self, model_name: str):
        self.model_name = model_name
        # In a real scenario, load the model here, e.g., SentenceTransformer(model_name)
        print(f"Initializing EmbeddingClient: Model={model_name}")

    async def get_embedding(self, text: str) -> List[float]:
        # Replace with actual embedding generation
        # print(f"Embedding text (mock): {text[:50]}...")
        # Simple hash-based mock embedding for consistent dimensions (e.g., 384 for all-MiniLM-L6-v2)
        import hashlib
        import numpy as np
        dim = 384 # Example dimension
        # Create a pseudo-random vector based on text hash
        hasher = hashlib.md5(text.encode())
        seed = int(hasher.hexdigest(), 16)
        rng = np.random.RandomState(seed)
        return rng.rand(dim).tolist()

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        # Batch embedding
        return [await self.get_embedding(text) for text in texts]
'''
from langchain_huggingface import HuggingFaceEmbeddings
EmbeddingClient = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")


# --- LangGraph State Definition ---
class InterviewGraphState(TypedDict):
    raw_transcript_path: str
    processed_transcript: Optional[List[DialogueTurn]]
    transcript_embeddings: Optional[Dict[int, List[float]]] # turn_id -> embedding
    
    assessment_criteria: List[Dict] # List of criteria dicts (from ASSESSMENT_CRITERIA)
    current_criterion_index: int
    current_criterion: Optional[Dict] # The criterion dict being processed
    
    topic_segments: Optional[List[TopicSegment]]
    # current_topic_id_for_assessment: Optional[str] # If we decide to assess topic by topic

    retrieved_contexts: Optional[List[RetrievedContext]] # Contexts for the current criterion
    
    individual_assessments: List[IndividualAssessment] # Accumulates assessments
    final_report_data: Optional[Dict] # For overall summary, strengths, weaknesses before shaping into pydantic model
    final_assessment_report: Optional[InterviewAssessmentReport]
    
    error_message: Optional[str]


# --- Example Usage of Preprocessing (for testing) ---
def load_sample_transcript(file_path="sample_transcript.json") -> List[Dict]:
    """Loads a sample transcript from a JSON file."""
    default_transcript = [
        {"speaker": "Interviewer", "start": "0:00:02", "end": "0:00:03", "text": "Welcome to the interview."},
        {"speaker": "Candidate", "start": "0:00:04", "end": "0:00:06", "text": "[Emotion: HAPPY] Thank you for having me!"},
        {"speaker": "Interviewer", "start": "0:00:07", "end": "0:00:10", "text": "Can you tell me about your experience with Project Alpha?"},
        {"speaker": "Candidate", "start": "0:00:11", "end": "0:00:20", "text": "[Emotion: NEUTRAL] Certainly. In Project Alpha, my main role was developing the backend API. It involved Python, FastAPI, and PostgreSQL. One of the key challenges was optimizing database queries for high traffic."},
        {"speaker": "Candidate", "start": "0:00:21", "end": "0:00:25", "text": "We managed to reduce latency by 30% by implementing better indexing strategies."},
        {"speaker": "Interviewer", "start": "0:00:26", "end": "0:00:28", "text": "That sounds impressive. What about teamwork?"},
        {"speaker": "Candidate", "start": "0:00:29", "end": "0:00:35", "text": "[Emotion: NEUTRAL] Teamwork was crucial. We had daily stand-ups and used Jira for task management. I believe in open communication."},
        {"speaker": "Interviewer", "start": "0:00:36", "end": "0:00:38", "text": "Okay. Do you have any questions for me?"},
        {"speaker": "Candidate", "start": "0:00:39", "end": "0:00:42", "text": "Yes, what does the typical career progression look like for this role?"},
        {"speaker": "Interviewer", "start": "0:00:43", "end": "0:00:45", "text": "That's a good question... (explains)"}
    ]
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Warning: Sample transcript file '{file_path}' not found. Using default transcript.")
        # Create the default file for next time
        with open(file_path, 'w') as f_out:
            json.dump(default_transcript, f_out, indent=2)
        return default_transcript
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from '{file_path}'. Using default transcript.")
        return default_transcript


async def main_test_preprocessing_and_models():
    print("--- Testing Preprocessing ---")
    raw_data = load_sample_transcript()
    processed_turns = preprocess_transcript(raw_data)
    for turn in processed_turns:
        print(f"Turn {turn.turn_id}: Speaker: {turn.speaker}, Emotion: {turn.emotion}, Clean Text: '{turn.clean_text[:50]}...'")

    print("\n--- Testing LLM Client (Topic Segmentation Mock) ---")
    # For actual LLM calls, ensure your chosen provider's libraries are installed (e.g., openai, anthropic)
    # and API keys are set as environment variables.
    # topic_llm = LLMClient
    # LLMClient
    sample_transcript_str = "\n".join([f"Turn {t.turn_id} ({t.speaker}): {t.clean_text}" for t in processed_turns])
    
    topic_prompt = f"""Segment the following interview dialogue into distinct topics.
    For each topic, provide a concise topic_label, start_turn_id, and end_turn_id.
    
    Transcript:
    {sample_transcript_str}
    """
    topic_llm = LLMClient.with_structured_output(TopicSegment)
    topics_json_str = topic_llm.invoke(
        [
            SystemMessage("You are an expert in analyzing conversational flow and identifying topics. Respond only with the requested JSON list."),
            HumanMessage(topic_prompt)
            ]
        )
    try:
        topic_segments_data = json.loads(topics_json_str)
        topic_segments = [TopicSegment(**topic_data) for topic_data in topic_segments_data]
        print("Parsed Topic Segments:")
        for topic in topic_segments:
            print(topic.model_dump_json(indent=2))
    except json.JSONDecodeError as e:
        print(f"Error parsing topic segments JSON: {e}")
        print(f"Received: {topics_json_str}")
    except Exception as e:
        print(f"An unexpected error occurred while parsing topics: {e}")


    print("\n--- Testing Embedding Client ---")
    # embed_client = EmbeddingClient
    sample_texts_to_embed = [turn.clean_text for turn in processed_turns if turn.clean_text]
    if sample_texts_to_embed:
        embeddings = [EmbeddingClient.embed_query(text) for text in sample_texts_to_embed]        
        print(f"Generated {len(embeddings)} embeddings. Dimension of first embedding: {len(embeddings[0]) if embeddings else 'N/A'}")
    else:
        print("No clean text found to embed.")

if __name__ == "__main__":
    import asyncio
    # To run the test:
    asyncio.run(main_test_preprocessing_and_models())
    # For now, just print a message that this is setup.
    # print("Core data structures, preprocessing functions, and mock clients defined.")
    # print("Next steps: Implement LangGraph nodes and the graph itself.")
    # Create a dummy sample_transcript.json if it doesn't exist
    # _ = load_sample_transcript()
    # print("sample_transcript.json ensured to exist for testing.")